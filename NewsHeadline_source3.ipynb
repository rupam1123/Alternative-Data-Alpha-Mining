{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dea22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publish_date      int64\n",
      "headline_text    object\n",
      "dtype: object\n",
      "Earliest article date: 2020-01-01\n",
      "Latest article date:   2020-12-31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40240, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Load dataset\n",
    "# df_news = pd.read_csv(\"/home/rupam/DataAlpha/data/raw/News_Headline/abcnews-date-text.csv\")\n",
    "\n",
    "# # Rename for consistency if needed\n",
    "# df_news.columns = df_news.columns.str.strip()\n",
    "\n",
    "# # Check actual column names and types\n",
    "# print(df_news.dtypes)\n",
    "\n",
    "# # Convert 'publish_date' from YYYYMMDD integer/string to datetime\n",
    "# df_news[\"publish_date\"] = pd.to_datetime(df_news[\"publish_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# # Filter for 2020 only\n",
    "# df_news_2020 = df_news[df_news[\"publish_date\"].dt.year == 2020].copy()\n",
    "\n",
    "# # Drop rows with missing headlines\n",
    "# df_news_2020 = df_news_2020[df_news_2020[\"headline_text\"].notna()]\n",
    "\n",
    "# # Optional: Save the cleaned file\n",
    "# df_news_2020.to_csv(\"/home/rupam/DataAlpha/data/raw/News_Headline/processed_news_2020.csv\", index=False)\n",
    "\n",
    "# # Check min/max date again\n",
    "# min_date = df_news_2020[\"publish_date\"].min()\n",
    "# max_date = df_news_2020[\"publish_date\"].max()\n",
    "\n",
    "# print(f\"Earliest article date: {min_date.date()}\")\n",
    "# print(f\"Latest article date:   {max_date.date()}\")\n",
    "# df_news_2020.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ffce14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentiment-enriched CSV saved to:\n",
      "/home/rupam/DataAlpha/data/raw/processed/news_sentiment_2020.csv\n",
      "Empty DataFrame\n",
      "Columns: [publish_date, headline_text, sentiment]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Use .loc to avoid SettingWithCopyWarning\n",
    "# df_news_2020.loc[:, \"sentiment\"] = df_news_2020[\"headline_text\"].apply(\n",
    "#     lambda x: analyzer.polarity_scores(str(x))[\"compound\"]\n",
    "# )\n",
    "# output_path = \"/home/rupam/DataAlpha/data/raw/processed/news_sentiment_2020.csv\"\n",
    "# df_news_2020.to_csv(output_path, index=False)\n",
    "\n",
    "# # Confirm\n",
    "# print(f\"✅ Sentiment-enriched CSV saved to:\\n{output_path}\")\n",
    "# print(df_news_2020[[\"publish_date\", \"headline_text\", \"sentiment\"]].head())\n",
    "\n",
    "# df_news_2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a785c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [publish_date, headline_text, sentiment, tickers]\n",
      "Index: []\n",
      "Number of filtered articles: 0\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# # Load your filtered 2020 news data with sentiment\n",
    "# df_news_2020 = pd.read_csv(\"/home/rupam/DataAlpha/data/raw/processed/news_sentiment_2020.csv\")\n",
    "\n",
    "# # Load S&P 500 tickers from Wikipedia\n",
    "# sp500 = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "# sp500_tickers = set(sp500[\"Symbol\"].tolist())\n",
    "\n",
    "# # Function to extract S&P 500 tickers from headline text\n",
    "# def extract_tickers(text):\n",
    "#     words = re.findall(r\"\\b[A-Z]{1,5}\\b\", str(text))\n",
    "#     return [w for w in words if w in sp500_tickers]\n",
    "\n",
    "# # Apply extraction\n",
    "# df_news_2020[\"tickers\"] = df_news_2020[\"headline_text\"].apply(extract_tickers)\n",
    "\n",
    "# # Keep only rows with at least one ticker mentioned\n",
    "# df_news_2020 = df_news_2020[df_news_2020[\"tickers\"].map(len) > 0]\n",
    "\n",
    "# # Reset index (optional)\n",
    "# df_news_2020 = df_news_2020.reset_index(drop=True)\n",
    "\n",
    "# # Save cleaned file\n",
    "# df_news_2020.to_csv(\"news_sentiment_2020_filtered.csv\", index=False)\n",
    "\n",
    "# # Preview\n",
    "# print(df_news_2020.head())\n",
    "# print(f\"Number of filtered articles: {len(df_news_2020)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12ae80ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved final sentiment file: /home/rupam/DataAlpha/data/raw/processed/news_sentiment_2020.csv\n",
      "         date ticker  sentiment\n",
      "0  2020-01-03   AMZN    -0.1779\n",
      "1  2020-01-08     BA    -0.4010\n",
      "2  2020-01-12   AMZN     0.1779\n",
      "3  2020-01-12     BA     0.0000\n",
      "4  2020-01-12    MMM     0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load the ABC News dataset\n",
    "# -----------------------------\n",
    "df_news = pd.read_csv(\"/home/rupam/DataAlpha/data/raw/News_Headline/abcnews-date-text.csv\")\n",
    "\n",
    "# Fix column names\n",
    "df_news.columns = df_news.columns.str.strip()\n",
    "\n",
    "# Convert publish_date to datetime (format is YYYYMMDD)\n",
    "df_news[\"publish_date\"] = pd.to_datetime(df_news[\"publish_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "\n",
    "# Filter for year 2020\n",
    "df_news_2020 = df_news[df_news[\"publish_date\"].dt.year == 2020].copy()\n",
    "\n",
    "# Drop empty headlines\n",
    "df_news_2020 = df_news_2020[df_news_2020[\"headline_text\"].notna()]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load S&P 500 Company Names and Tickers\n",
    "# -----------------------------\n",
    "wiki_table = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "\n",
    "# Create a mapping from lowercase company name to ticker\n",
    "company_to_ticker = {\n",
    "    row[\"Security\"].lower(): row[\"Symbol\"]\n",
    "    for _, row in wiki_table.iterrows()\n",
    "}\n",
    "\n",
    "company_names = list(company_to_ticker.keys())\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Match company name in headline\n",
    "# -----------------------------\n",
    "def extract_tickers_from_company_names(headline):\n",
    "    headline_lower = headline.lower()\n",
    "    matched = []\n",
    "    for company in company_names:\n",
    "        if company in headline_lower:\n",
    "            matched.append(company_to_ticker[company])\n",
    "    return list(set(matched))  # remove duplicates\n",
    "\n",
    "df_news_2020[\"tickers\"] = df_news_2020[\"headline_text\"].apply(extract_tickers_from_company_names)\n",
    "\n",
    "# Keep only rows with at least one ticker match\n",
    "df_news_2020 = df_news_2020[df_news_2020[\"tickers\"].map(len) > 0].copy()\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Apply VADER Sentiment\n",
    "# -----------------------------\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df_news_2020[\"sentiment\"] = df_news_2020[\"headline_text\"].apply(\n",
    "    lambda x: analyzer.polarity_scores(str(x))[\"compound\"]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Expand to one row per ticker\n",
    "# -----------------------------\n",
    "expanded_rows = []\n",
    "for _, row in df_news_2020.iterrows():\n",
    "    for ticker in row[\"tickers\"]:\n",
    "        expanded_rows.append({\n",
    "            \"date\": row[\"publish_date\"].date(),\n",
    "            \"ticker\": ticker,\n",
    "            \"sentiment\": row[\"sentiment\"],\n",
    "            \"headline\": row[\"headline_text\"]\n",
    "        })\n",
    "\n",
    "df_expanded = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Aggregate sentiment by date & ticker\n",
    "# -----------------------------\n",
    "df_sentiment_daily = df_expanded.groupby([\"date\", \"ticker\"]).agg({\n",
    "    \"sentiment\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Save final CSV\n",
    "# -----------------------------\n",
    "output_path = \"/home/rupam/DataAlpha/data/raw/processed/news_sentiment_2020.csv\"\n",
    "df_sentiment_daily.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Saved final sentiment file: {output_path}\")\n",
    "print(df_sentiment_daily.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2600e683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tickers matched in news: 28\n",
      "Some of the tickers found: ['AMZN' 'BA' 'MMM' 'NFLX' 'CMI' 'MSFT' 'CSCO' 'ABNB' 'WFC' 'UBER' 'MAS'\n",
      " 'EFX' 'CCL' 'INTC' 'GM' 'APTV' 'COST' 'HOLX' 'DD' 'PFE' 'MRNA' 'STE'\n",
      " 'AMCR' 'GILD' 'WM' 'LEN' 'LDOS' 'WMT']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total unique tickers matched in news: {df_sentiment_daily['ticker'].nunique()}\")\n",
    "print(\"Some of the tickers found:\", df_sentiment_daily['ticker'].unique()[:28])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
