{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25887/206365085.py:4: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"/home/rupam/DataAlpha/data/raw/social_media/r_wallstreetbets_posts.csv\")  # replace with actual path\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              created_date                                              title\n",
      "579370 2020-12-31 23:57:53                         Selling Leaps - Vega Plays\n",
      "579371 2020-12-31 23:57:47     ‚Äú2021 is gonna be different.I can feel it.‚ÄùüöÄüöÄüöÄ\n",
      "579372 2020-12-31 23:57:47  Don't work for your money, make your money wor...\n",
      "579373 2020-12-31 23:57:13             Those GE 100$ calls didn‚Äôt work out...\n",
      "579374 2020-12-31 23:56:40                                 Some advice needed\n",
      "Number of posts from 2020: 264758\n",
      "(264758, 13)\n",
      "Index(['id', 'title', 'score', 'author', 'author_flair_text', 'removed_by',\n",
      "       'total_awards_received', 'awarders', 'created_utc', 'full_link',\n",
      "       'num_comments', 'over_18', 'created_date'],\n",
      "      dtype='object')\n",
      "            id                                              title  score  \\\n",
      "579370  ko0zd1                         Selling Leaps - Vega Plays      1   \n",
      "579371  ko0zb8     ‚Äú2021 is gonna be different.I can feel it.‚ÄùüöÄüöÄüöÄ      1   \n",
      "579372  ko0zau  Don't work for your money, make your money wor...      1   \n",
      "\n",
      "                    author  author_flair_text removed_by  \\\n",
      "579370  FatCatBoomerBanker  SUPREME COMMANDER        NaN   \n",
      "579371              sam4gh                NaN        NaN   \n",
      "579372             WACS_On                NaN        NaN   \n",
      "\n",
      "        total_awards_received awarders  created_utc  \\\n",
      "579370                    0.0       []   1609459073   \n",
      "579371                    0.0       []   1609459067   \n",
      "579372                    0.0       []   1609459067   \n",
      "\n",
      "                                                full_link  num_comments  \\\n",
      "579370  https://www.reddit.com/r/wallstreetbets/commen...            48   \n",
      "579371  https://www.reddit.com/r/wallstreetbets/commen...             3   \n",
      "579372  https://www.reddit.com/r/wallstreetbets/commen...             5   \n",
      "\n",
      "        over_18        created_date  \n",
      "579370    False 2020-12-31 23:57:53  \n",
      "579371    False 2020-12-31 23:57:47  \n",
      "579372    False 2020-12-31 23:57:47  \n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# df = pd.read_csv(\"/home/rupam/DataAlpha/data/raw/social_media/r_wallstreetbets_posts.csv\")  # replace with actual path\n",
    "# # Convert timestamp to readable datetime\n",
    "\n",
    "# df[\"created_date\"] = pd.to_datetime(df[\"created_utc\"], unit='s')\n",
    "# # Filter only 2020 posts\n",
    "\n",
    "# df_2020 = df[df[\"created_date\"].dt.year == 2020]\n",
    "\n",
    "# # Optional: Save to a new CSV for later use\n",
    "# df_2020.to_csv(\"/home/rupam/DataAlpha/data/raw/social_media/df_2020.csv\", index=False)\n",
    "\n",
    "# print(df_2020[[\"created_date\", \"title\"]].head())\n",
    "# # Drop posts with missing titles\n",
    "# df_2020 = df_2020[df_2020[\"title\"].notna()]\n",
    "# # Optional: Remove duplicates\n",
    "# df_2020 = df_2020.drop_duplicates(subset=\"title\")\n",
    "# # Check how many posts\n",
    "# print(f\"Number of posts from 2020: {len(df_2020)}\")\n",
    "# print(df_2020.shape)\n",
    "# print(df_2020.columns)\n",
    "# print(df_2020.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fafbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TSLA', 6542), ('DD', 4777), ('PLTR', 4111), ('A', 3945), ('MSFT', 1581), ('AMD', 1534), ('AAPL', 1236), ('ON', 781), ('AMZN', 778), ('F', 731), ('ALL', 581), ('DIS', 566), ('T', 565), ('BA', 493), ('IT', 444), ('J', 438), ('C', 418), ('NVDA', 411), ('NOW', 404), ('CCL', 366), ('V', 349), ('PSA', 329), ('MRNA', 297), ('ARE', 294), ('GILD', 271), ('D', 265), ('WMT', 260), ('MGM', 247), ('NFLX', 247), ('GE', 244), ('PFE', 230), ('DOW', 228), ('UPS', 202), ('GM', 197), ('INTC', 195), ('L', 187), ('UBER', 183), ('O', 173), ('DAY', 165), ('PCG', 160), ('RCL', 157), ('CRM', 153), ('DAL', 149), ('SBUX', 135), ('BAC', 132), ('NCLH', 128), ('K', 121), ('JPM', 118), ('UAL', 116), ('PM', 109), ('CVS', 108), ('COST', 107), ('LULU', 105), ('HAS', 103), ('MU', 102), ('HD', 97), ('TGT', 95), ('FDX', 94), ('SO', 91), ('LOW', 88), ('CSCO', 87), ('ABNB', 84), ('GOOG', 81), ('KO', 80), ('ENPH', 78), ('WYNN', 77), ('MCD', 70), ('PG', 69), ('CZR', 69), ('ES', 68), ('XOM', 68), ('CRWD', 67), ('JNJ', 64), ('GOOGL', 61), ('DASH', 61), ('ULTA', 61), ('LMT', 60), ('CLX', 60), ('OXY', 58), ('KR', 57), ('WFC', 56), ('NKE', 54), ('LUV', 54), ('LYV', 54), ('ORCL', 50), ('SPG', 48), ('PYPL', 48), ('ADBE', 47), ('LVS', 47), ('TMUS', 46), ('MA', 44), ('RTX', 44), ('DPZ', 44), ('MMM', 43), ('DG', 42), ('ABT', 42), ('IBM', 42), ('TECH', 41), ('EA', 41), ('ABBV', 41), ('REGN', 40), ('CMG', 40), ('MTCH', 39), ('HPQ', 39), ('BBY', 37), ('FE', 36), ('EBAY', 36), ('MS', 35), ('TTWO', 35), ('FANG', 35), ('GIS', 34), ('MAR', 33), ('GS', 32), ('PEP', 32), ('QCOM', 31), ('MO', 31), ('UNH', 30), ('AMAT', 29), ('BIIB', 29), ('YUM', 28), ('DLTR', 28), ('FAST', 28), ('WM', 28), ('WBA', 27), ('DTE', 27), ('CPB', 27), ('TSN', 26), ('EXPE', 26), ('HPE', 25), ('DGX', 25), ('STZ', 25), ('FSLR', 23), ('CME', 23), ('CAT', 23), ('BRO', 23), ('VZ', 22), ('FCX', 21), ('KMX', 21), ('DELL', 21), ('AXP', 20), ('TJX', 20), ('MDT', 20), ('WELL', 19), ('KHC', 19), ('CL', 19), ('DXCM', 19), ('HAL', 19), ('ETN', 19), ('GLW', 17), ('NEE', 17), ('TMO', 16), ('NOC', 16), ('SYY', 16), ('KMB', 15), ('BKNG', 15), ('ICE', 14), ('COO', 14), ('WDAY', 14), ('CBOE', 14), ('TAP', 13), ('DE', 13), ('PANW', 13), ('MRK', 13), ('BMY', 12), ('COIN', 12), ('ADP', 12), ('IP', 12), ('AVGO', 12), ('BK', 12), ('HLT', 12), ('HON', 11), ('LLY', 11), ('DRI', 11), ('HOLX', 11), ('VTRS', 10), ('DUK', 10), ('CMCSA', 10), ('UNP', 10), ('FOX', 10), ('FTNT', 9), ('STLD', 9), ('MCK', 9), ('KEY', 9), ('CVX', 9), ('BLK', 9), ('APA', 9), ('LH', 9), ('DHR', 9), ('PAYX', 9), ('EL', 9), ('INTU', 9), ('EOG', 9), ('CB', 8), ('NEM', 8), ('CARR', 8), ('WDC', 8), ('MLM', 8), ('LEN', 8), ('HCA', 8), ('NDAQ', 8), ('ADSK', 8), ('MDLZ', 8), ('CBRE', 8), ('ISRG', 7), ('ORLY', 7), ('SJM', 7), ('TXN', 7), ('AMCR', 7), ('ALGN', 7), ('COF', 7), ('LRCX', 7), ('MNST', 7), ('SCHW', 7), ('JBL', 7), ('BR', 7), ('MSCI', 7), ('META', 6), ('ED', 6), ('PGR', 6), ('AIG', 6), ('VRTX', 6), ('STX', 6), ('GRMN', 6), ('CHTR', 6), ('ROST', 6), ('TER', 6), ('UHS', 6), ('ADI', 6), ('APO', 6), ('DHI', 5), ('SW', 5), ('BDX', 5), ('ZBRA', 5), ('LIN', 5), ('MPC', 5), ('CI', 5), ('NTAP', 5), ('ALB', 5), ('AKAM', 5), ('SHW', 5), ('TPR', 5), ('MCHP', 5), ('OKE', 5), ('SLB', 5), ('PCAR', 5), ('HIG', 5), ('LDOS', 5), ('GL', 4), ('DVN', 4), ('GNRC', 4), ('LII', 4), ('CAH', 4), ('TXT', 4), ('POOL', 4), ('FDS', 4), ('PKG', 4), ('HUM', 4), ('CTAS', 4), ('CHD', 4), ('USB', 4), ('TDG', 4), ('VICI', 4), ('MET', 4), ('SYF', 4), ('GD', 4), ('CSX', 4), ('VLO', 3), ('AES', 3), ('VST', 3), ('CHRW', 3), ('CF', 3), ('MOS', 3), ('DOC', 3), ('BALL', 3), ('BG', 3), ('EW', 3), ('MAS', 3), ('WY', 3), ('PHM', 3), ('MKC', 3), ('KLAC', 3), ('HSY', 3), ('PNW', 3), ('PTC', 3), ('GEN', 3), ('SPGI', 3), ('REG', 3), ('VMC', 3), ('KEYS', 3), ('GDDY', 3), ('MSI', 3), ('FITB', 3), ('CINF', 3), ('ECL', 3), ('WST', 3), ('ANET', 3), ('URI', 3), ('CPRT', 3), ('VTR', 2), ('AWK', 2), ('TT', 2), ('APD', 2), ('OTIS', 2), ('EMN', 2), ('EMR', 2), ('XEL', 2), ('NI', 2), ('FFIV', 2), ('KKR', 2), ('COP', 2), ('JBHT', 2), ('PH', 2), ('FI', 2), ('SWKS', 2), ('PPL', 2), ('MHK', 2), ('IR', 2), ('LHX', 2), ('DVA', 2), ('APTV', 2), ('TSCO', 2), ('PSX', 2), ('CAG', 2), ('AZO', 2), ('IVZ', 2), ('HRL', 2), ('PNC', 2), ('HBAN', 2), ('KIM', 2), ('CTSH', 2), ('PPG', 2), ('HST', 2), ('LKQ', 2), ('OMC', 2), ('BSX', 2), ('HSIC', 2), ('AMT', 2), ('APH', 2), ('NUE', 2), ('MCO', 2), ('LYB', 2), ('CCI', 2), ('BEN', 2), ('EQT', 1), ('ATO', 1), ('AMGN', 1), ('ALLE', 1), ('PWR', 1), ('BX', 1), ('BXP', 1), ('SMCI', 1), ('KDP', 1), ('EQR', 1), ('WAB', 1), ('EQIX', 1), ('DLR', 1), ('COR', 1), ('DECK', 1), ('WSM', 1), ('AFL', 1), ('IQV', 1), ('GPN', 1), ('NWS', 1), ('SYK', 1), ('AME', 1), ('LW', 1), ('PNR', 1), ('CDNS', 1), ('JNPR', 1), ('TKO', 1), ('WMB', 1), ('HES', 1), ('AXON', 1), ('ETR', 1), ('IRM', 1), ('NWSA', 1), ('MMC', 1), ('KMI', 1), ('ZTS', 1), ('WEC', 1), ('MPWR', 1), ('SBAC', 1), ('EXC', 1), ('ROP', 1), ('HWM', 1), ('TFC', 1), ('NSC', 1), ('ODFL', 1), ('JCI', 1), ('RL', 1), ('ACN', 1), ('PAYC', 1), ('XYL', 1), ('CMS', 1), ('ITW', 1), ('AVB', 1), ('AMP', 1), ('AON', 1), ('WAT', 1), ('EXPD', 1), ('ROK', 1), ('MAA', 1), ('MTD', 1), ('FOXA', 1), ('NXPI', 1), ('IDXX', 1), ('STT', 1), ('STE', 1)]\n",
      "408\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Scrape S&P 500 tickers from Wikipedia\n",
    "# url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "# table = pd.read_html(url)[0]  # First table on the page\n",
    "\n",
    "# # Get tickers from the Symbol column\n",
    "# sp500_tickers = set(table[\"Symbol\"].tolist())\n",
    "\n",
    "# import re\n",
    "# from collections import Counter\n",
    "\n",
    "# tickers = []\n",
    "\n",
    "# for title in df_2020[\"title\"].dropna():\n",
    "#     words = re.findall(r\"\\b[A-Z]{1,5}\\b\", title)\n",
    "#     tickers.extend(words)\n",
    "\n",
    "# # Count frequency\n",
    "# all_counts = Counter(tickers)\n",
    "\n",
    "# # Filter only those in known tickers\n",
    "# cleaned_counts = {ticker: count for ticker, count in all_counts.items() if ticker in sp500_tickers}\n",
    "\n",
    "# # Display top 20\n",
    "# print(sorted(cleaned_counts.items(), key=lambda x: -x[1])[:500])\n",
    "\n",
    "\n",
    "# print(len(cleaned_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f150cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# # Function to extract S&P 500 tickers from title\n",
    "# def extract_valid_tickers(title):\n",
    "#     words = re.findall(r\"\\b[A-Z]{1,5}\\b\", str(title))\n",
    "#     return [word for word in words if word in sp500_tickers]\n",
    "\n",
    "# df_2020[\"tickers\"] = df_2020[\"title\"].apply(extract_valid_tickers)\n",
    "\n",
    "# # Keep only posts that mention at least one valid ticker\n",
    "# df_2020 = df_2020[df_2020[\"tickers\"].map(len) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3784a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37214, 15)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# df_2020[\"sentiment\"] = df_2020[\"title\"].apply(lambda x: analyzer.polarity_scores(str(x))[\"compound\"])\n",
    "# df_2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dabe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Expand to one row per (ticker, post)\n",
    "# rows = []\n",
    "\n",
    "# for _, row in df_2020.iterrows():\n",
    "#     for ticker in row[\"tickers\"]:\n",
    "#         rows.append({\n",
    "#             \"date\": row[\"date\"].date(),  # only date part\n",
    "#             \"ticker\": ticker,\n",
    "#             \"sentiment\": row[\"sentiment\"]\n",
    "#         })\n",
    "\n",
    "# df_exploded = pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6510e095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25887/1899384348.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(DATA_PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of 2020 posts:\n",
      "              created_date                                              title\n",
      "579370 2020-12-31 23:57:53                         Selling Leaps - Vega Plays\n",
      "579371 2020-12-31 23:57:47     ‚Äú2021 is gonna be different.I can feel it.‚ÄùüöÄüöÄüöÄ\n",
      "579372 2020-12-31 23:57:47  Don't work for your money, make your money wor...\n",
      "579373 2020-12-31 23:57:13             Those GE 100$ calls didn‚Äôt work out...\n",
      "579374 2020-12-31 23:56:40                                 Some advice needed\n",
      "Number of posts from 2020: 286130\n",
      "Columns: ['id', 'title', 'score', 'author', 'author_flair_text', 'removed_by', 'total_awards_received', 'awarders', 'created_utc', 'full_link', 'num_comments', 'over_18', 'created_date']\n",
      "Total S&P 500 tickers loaded: 503\n",
      "Top 20 extracted tickers: [('TSLA', 6542), ('DD', 4777), ('PLTR', 4111), ('A', 3945), ('MSFT', 1581), ('AMD', 1534), ('AAPL', 1236), ('ON', 781), ('AMZN', 778), ('F', 731), ('ALL', 581), ('DIS', 566), ('T', 565), ('BA', 493), ('IT', 444), ('J', 438), ('C', 418), ('NVDA', 411), ('NOW', 404), ('CCL', 366)]\n",
      "Unique valid tickers found: 408\n",
      "Aggregated daily sentiment sample:\n",
      "         date ticker  sentiment\n",
      "0  2020-01-01      A    -0.0640\n",
      "1  2020-01-01    AMD     0.0000\n",
      "2  2020-01-01     DD     0.0624\n",
      "3  2020-01-01   INTU     0.0000\n",
      "4  2020-01-01     KO     0.0000\n",
      "Final processed data saved to: /home/rupam/DataAlpha/data/raw/processed/reddit_sentiment_2020.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# -----------------------\n",
    "# 1. Load Reddit Dataset\n",
    "# -----------------------\n",
    "# Update the path below as needed.\n",
    "DATA_PATH = \"/home/rupam/DataAlpha/data/raw/social_media/r_wallstreetbets_posts.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Convert UNIX Timestamp to Date\n",
    "# -------------------------------\n",
    "# Assume the column \"created_utc\" stores UNIX timestamps.\n",
    "df[\"created_date\"] = pd.to_datetime(df[\"created_utc\"], unit='s')\n",
    "\n",
    "# -------------------------------------\n",
    "# 3. Filter Dataset for the Year 2020\n",
    "# -------------------------------------\n",
    "df_2020 = df[df[\"created_date\"].dt.year == 2020].copy()\n",
    "\n",
    "# (Optional) Save the raw 2020 posts for future use\n",
    "OUTPUT_2020_PATH = \"/home/rupam/DataAlpha/data/raw/social_media/df_2020.csv\"\n",
    "df_2020.to_csv(OUTPUT_2020_PATH, index=False)\n",
    "\n",
    "# Preview a few rows\n",
    "print(\"Preview of 2020 posts:\")\n",
    "print(df_2020[[\"created_date\", \"title\"]].head())\n",
    "print(f\"Number of posts from 2020: {len(df_2020)}\")\n",
    "print(\"Columns:\", df_2020.columns.tolist())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. Clean Data: Remove Posts with Missing Titles & Duplicates\n",
    "# ----------------------------------------------------\n",
    "df_2020 = df_2020[df_2020[\"title\"].notna()].copy()\n",
    "df_2020 = df_2020.drop_duplicates(subset=\"title\")\n",
    "\n",
    "# -------------------------------------------\n",
    "# 5. Load S&P 500 Tickers from Wikipedia\n",
    "# -------------------------------------------\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "# Read the first table from Wikipedia containing S&P 500 companies\n",
    "sp500_table = pd.read_html(url)[0]\n",
    "# Extract tickers from the \"Symbol\" column and convert to a set\n",
    "sp500_tickers = set(sp500_table[\"Symbol\"].tolist())\n",
    "\n",
    "print(f\"Total S&P 500 tickers loaded: {len(sp500_tickers)}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6. Define a Function to Extract Valid Tickers from Title\n",
    "# ----------------------------------------------------\n",
    "def extract_valid_tickers(title):\n",
    "    # Find all words that are 1-5 uppercase letters\n",
    "    words = re.findall(r\"\\b[A-Z]{1,5}\\b\", str(title))\n",
    "    # Filter words based on membership in the S&P 500 tickers set\n",
    "    return [word for word in words if word in sp500_tickers]\n",
    "\n",
    "# Use .loc to safely assign new column \"tickers\" to avoid SettingWithCopyWarning.\n",
    "df_2020.loc[:, \"tickers\"] = df_2020[\"title\"].apply(extract_valid_tickers)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Filter Only Posts That Mention at Least One Ticker\n",
    "# --------------------------------------------------\n",
    "df_2020 = df_2020[df_2020[\"tickers\"].map(len) > 0].copy()\n",
    "\n",
    "# (Optional) Display frequency counts to see which tickers are most common:\n",
    "all_extracted = []\n",
    "for tlist in df_2020[\"tickers\"]:\n",
    "    all_extracted.extend(tlist)\n",
    "frequency_counts = Counter(all_extracted)\n",
    "print(\"Top 20 extracted tickers:\", frequency_counts.most_common(20))\n",
    "print(f\"Unique valid tickers found: {len(frequency_counts)}\")\n",
    "\n",
    "# -------------------------------------\n",
    "# 8. Compute Sentiment Using VADER on Titles\n",
    "# -------------------------------------\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "df_2020.loc[:, \"sentiment\"] = df_2020[\"title\"].apply(\n",
    "    lambda x: analyzer.polarity_scores(str(x))[\"compound\"]\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 9. (Optional) Expand Posts to One Row per Ticker\n",
    "# ---------------------------------------------\n",
    "# Many posts mention multiple tickers. We expand the data so each row corresponds to one ticker.\n",
    "expanded_rows = []\n",
    "for _, row in df_2020.iterrows():\n",
    "    for ticker in row[\"tickers\"]:\n",
    "        expanded_rows.append({\n",
    "            \"date\": row[\"created_date\"].date(),  # Save just the date part\n",
    "            \"ticker\": ticker,\n",
    "            \"sentiment\": row[\"sentiment\"],\n",
    "            \"title\": row[\"title\"]  # optional: keep the original title for reference\n",
    "        })\n",
    "\n",
    "df_expanded = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 10. Aggregate Sentiment by Date and Ticker\n",
    "# ---------------------------------------------\n",
    "# For each date and ticker, compute the average sentiment.\n",
    "df_sentiment_daily = df_expanded.groupby([\"date\", \"ticker\"]).agg({\n",
    "    \"sentiment\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Aggregated daily sentiment sample:\")\n",
    "print(df_sentiment_daily.head())\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 11. Save the Final Processed Data for Later Model Training\n",
    "# -------------------------------------------------------------\n",
    "OUTPUT_PROCESSED_PATH = \"/home/rupam/DataAlpha/data/raw/processed/reddit_sentiment_2020.csv\"\n",
    "df_sentiment_daily.to_csv(OUTPUT_PROCESSED_PATH, index=False)\n",
    "\n",
    "print(f\"Final processed data saved to: {OUTPUT_PROCESSED_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
