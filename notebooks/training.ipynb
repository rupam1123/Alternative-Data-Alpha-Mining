{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a569558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 49084, number of negative: 45360\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3435\n",
      "[LightGBM] [Info] Number of data points in the train set: 94444, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "🔍 Evaluation of Confident Predictions Only\n",
      "\n",
      "📊 Random Forest | Thresholds: low=0.35, high=0.65\n",
      "Confident Predictions: 89 / 23612\n",
      "[[28 16]\n",
      " [41  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.64      0.50        44\n",
      "           1       0.20      0.09      0.12        45\n",
      "\n",
      "    accuracy                           0.36        89\n",
      "   macro avg       0.30      0.36      0.31        89\n",
      "weighted avg       0.30      0.36      0.31        89\n",
      "\n",
      "\n",
      "📊 LightGBM | Thresholds: low=0.35, high=0.65\n",
      "Confident Predictions: 806 / 23612\n",
      "[[225 127]\n",
      " [259 195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.64      0.54       352\n",
      "           1       0.61      0.43      0.50       454\n",
      "\n",
      "    accuracy                           0.52       806\n",
      "   macro avg       0.54      0.53      0.52       806\n",
      "weighted avg       0.54      0.52      0.52       806\n",
      "\n",
      "\n",
      "📊 Random Forest | Thresholds: low=0.4, high=0.6\n",
      "Confident Predictions: 470 / 23612\n",
      "[[169  65]\n",
      " [189  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.72      0.57       234\n",
      "           1       0.42      0.20      0.27       236\n",
      "\n",
      "    accuracy                           0.46       470\n",
      "   macro avg       0.45      0.46      0.42       470\n",
      "weighted avg       0.45      0.46      0.42       470\n",
      "\n",
      "\n",
      "📊 LightGBM | Thresholds: low=0.4, high=0.6\n",
      "Confident Predictions: 2195 / 23612\n",
      "[[628 340]\n",
      " [692 535]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.65      0.55       968\n",
      "           1       0.61      0.44      0.51      1227\n",
      "\n",
      "    accuracy                           0.53      2195\n",
      "   macro avg       0.54      0.54      0.53      2195\n",
      "weighted avg       0.55      0.53      0.53      2195\n",
      "\n",
      "\n",
      "📊 Random Forest | Thresholds: low=0.45, high=0.55\n",
      "Confident Predictions: 2813 / 23612\n",
      "[[872 385]\n",
      " [961 595]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.69      0.56      1257\n",
      "           1       0.61      0.38      0.47      1556\n",
      "\n",
      "    accuracy                           0.52      2813\n",
      "   macro avg       0.54      0.54      0.52      2813\n",
      "weighted avg       0.55      0.52      0.51      2813\n",
      "\n",
      "\n",
      "📊 LightGBM | Thresholds: low=0.45, high=0.55\n",
      "Confident Predictions: 6707 / 23612\n",
      "[[1666 1394]\n",
      " [1756 1891]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.54      0.51      3060\n",
      "           1       0.58      0.52      0.55      3647\n",
      "\n",
      "    accuracy                           0.53      6707\n",
      "   macro avg       0.53      0.53      0.53      6707\n",
      "weighted avg       0.54      0.53      0.53      6707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1️⃣ Load preprocessed feature data\n",
    "df = pd.read_csv(r\"C:\\Users\\mrity\\OneDrive\\Desktop\\Alternative-Data-Alpha-Mining\\data\\raw\\processed\\ffff.csv\", parse_dates=['date'])\n",
    "\n",
    "# 2️⃣ Advanced Feature Engineering\n",
    "df['reddit_filtered'] = df['reddit_sentiment'].where(df['reddit_sentiment'].abs() > 0.2, 0)\n",
    "df['news_filtered'] = df['news_sentiment'].where(df['news_sentiment'].abs() > 0.2, 0)\n",
    "# Ensure roll3 and roll7 exist\n",
    "for col in ['trend_score', 'reddit_sentiment', 'news_sentiment']:\n",
    "    df[f'{col}_roll3'] = df.groupby('ticker')[col].transform(lambda x: x.shift(1).rolling(3, min_periods=1).mean())\n",
    "    df[f'{col}_roll7'] = df.groupby('ticker')[col].transform(lambda x: x.shift(1).rolling(7, min_periods=1).mean())\n",
    "\n",
    "# Now momentum works\n",
    "df['trend_momentum'] = df['trend_score_roll3'] - df['trend_score_roll7']\n",
    "\n",
    "df['trend_momentum'] = df['trend_score_roll3'] - df['trend_score_roll7']\n",
    "df['return_roll3'] = df.groupby('ticker')['return'].transform(lambda x: x.shift(1).rolling(3).mean())\n",
    "df['return_delta'] = df.groupby('ticker')['return'].diff()\n",
    "df['signal_agreement'] = ((df['reddit_sentiment'] > 0) & (df['trend_score'] > 0)).astype(int) - \\\n",
    "                         ((df['reddit_sentiment'] < 0) & (df['trend_score'] < 0)).astype(int)\n",
    "# ⏱ Rolling volatility (risk)\n",
    "df['return_volatility_5'] = df.groupby('ticker')['return'].transform(lambda x: x.shift(1).rolling(5).std())\n",
    "\n",
    "\n",
    "# 🧠 Interaction terms\n",
    "df['reddit_trend_agree'] = df['reddit_sentiment'] * df['trend_score']\n",
    "df['news_trend_agree'] = df['news_sentiment'] * df['trend_score']\n",
    "\n",
    "# 3️⃣ Clean and prepare\n",
    "exclude_cols = ['date', 'ticker', 'return', 'target_return', 'target_up']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "df = df.dropna(subset=feature_cols + ['target_up'])\n",
    "X = df[feature_cols]\n",
    "y = df['target_up']\n",
    "\n",
    "# 4️⃣ Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 5️⃣ Class weights\n",
    "weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.array([0,1]), y=y_train)\n",
    "class_weights_dict = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "# 6️⃣ Train Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=1,\n",
    "    class_weight=class_weights_dict,\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_probs = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 7️⃣ Feature Importance & Selection (Top 15)\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_cols)\n",
    "top_features = importances.sort_values(ascending=False).head(15).index.tolist()\n",
    "X_train_sel = X_train[top_features]\n",
    "X_test_sel = X_test[top_features]\n",
    "\n",
    "# 8️⃣ Train LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(class_weight='balanced', random_state=42)\n",
    "lgb_model.fit(X_train_sel, y_train)\n",
    "lgb_probs = lgb_model.predict_proba(X_test_sel)[:, 1]\n",
    "\n",
    "# 9️⃣ Evaluate at Different Thresholds\n",
    "thresholds = [(0.35, 0.65), (0.4, 0.6), (0.45, 0.55)]\n",
    "\n",
    "print(\"🔍 Evaluation of Confident Predictions Only\")\n",
    "for low, high in thresholds:\n",
    "    for name, probs in [('Random Forest', rf_probs), ('LightGBM', lgb_probs)]:\n",
    "        confident_up = probs > high\n",
    "        confident_down = probs < low\n",
    "        confident_mask = confident_up | confident_down\n",
    "        y_pred = np.where(confident_up, 1, 0)\n",
    "        y_true = y_test[confident_mask]\n",
    "        y_pred = y_pred[confident_mask]\n",
    "        print(f\"\\n📊 {name} | Thresholds: low={low}, high={high}\")\n",
    "        print(f\"Confident Predictions: {len(y_true)} / {len(y_test)}\")\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23e2d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def rolling_backtest_calendar(df, feature_cols, label_col='target_up',\n",
    "                              train_days=60, test_days=20,\n",
    "                              thresholds=(0.35, 0.65)):\n",
    "\n",
    "    df = df.sort_values(['date']).reset_index(drop=True)\n",
    "    unique_dates = df['date'].sort_values().unique()\n",
    "    all_results = []\n",
    "\n",
    "    for start_idx in range(0, len(unique_dates) - train_days - test_days + 1, test_days):\n",
    "        # Calendar-based rolling window\n",
    "        train_start = unique_dates[start_idx]\n",
    "        train_end = unique_dates[start_idx + train_days - 1]\n",
    "        test_start = unique_dates[start_idx + train_days]\n",
    "        test_end = unique_dates[start_idx + train_days + test_days - 1]\n",
    "\n",
    "        train_df = df[(df['date'] >= train_start) & (df['date'] <= train_end)]\n",
    "        test_df = df[(df['date'] >= test_start) & (df['date'] <= test_end)]\n",
    "\n",
    "        if train_df.empty or test_df.empty:\n",
    "            continue\n",
    "\n",
    "        X_train_full = train_df[feature_cols]\n",
    "        y_train = train_df[label_col]\n",
    "        X_test_full = test_df[feature_cols]\n",
    "        y_test = test_df[label_col].values\n",
    "        tickers = test_df['ticker'].values\n",
    "        test_dates = test_df['date'].values\n",
    "\n",
    "        # Skip if one class only\n",
    "        if len(np.unique(y_train)) == 1:\n",
    "            continue\n",
    "\n",
    "        # Compute class weights\n",
    "        weights = class_weight.compute_class_weight('balanced', classes=np.array([0,1]), y=y_train)\n",
    "        class_weights_dict = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "        # Feature selection: initial RF\n",
    "        rf_initial = RandomForestClassifier(n_estimators=100, max_depth=10,\n",
    "                                            min_samples_split=5, min_samples_leaf=1,\n",
    "                                            class_weight=class_weights_dict, random_state=42)\n",
    "        rf_initial.fit(X_train_full, y_train)\n",
    "        top_features = pd.Series(rf_initial.feature_importances_, index=feature_cols)\\\n",
    "                        .sort_values(ascending=False).head(15).index.tolist()\n",
    "\n",
    "        # Retrain final RF with top features\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=10,\n",
    "                                    min_samples_split=5, min_samples_leaf=1,\n",
    "                                    class_weight=class_weights_dict, random_state=42)\n",
    "        rf.fit(X_train_full[top_features], y_train)\n",
    "        rf_probs = rf.predict_proba(X_test_full[top_features])[:, 1]\n",
    "\n",
    "        # Confident predictions\n",
    "        low, high = thresholds\n",
    "        confident_up = rf_probs > high\n",
    "        confident_down = rf_probs < low\n",
    "        confident_mask = confident_up | confident_down\n",
    "        if confident_mask.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        y_pred_conf = np.where(confident_up[confident_mask], 1, 0)\n",
    "        y_true_conf = y_test[confident_mask]\n",
    "        date_conf = test_dates[confident_mask]\n",
    "        ticker_conf = tickers[confident_mask]\n",
    "        target_returns = test_df['target_return'].values[confident_mask]\n",
    "\n",
    "        for p, t, d, tk, r in zip(y_pred_conf, y_true_conf, date_conf, ticker_conf, target_returns):\n",
    "            all_results.append({\n",
    "                'date': d, 'ticker': tk, 'pred': p, 'true': t,\n",
    "                'target_return': r, 'pnl': r * (1 if p == 1 else -1)\n",
    "            })\n",
    "\n",
    "        print(f\"✅ Trained {train_start} → {train_end} | Tested {test_start} → {test_end} | Trades: {len(y_pred_conf)}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "def compute_portfolio_metrics(results_df, capital=100000):\n",
    "    results_df['date'] = pd.to_datetime(results_df['date'])\n",
    "    daily_pnl = results_df.groupby('date')['pnl'].mean()  # equally weighted across tickers\n",
    "    cumulative = (1 + daily_pnl).cumprod()\n",
    "\n",
    "    # Sharpe (annualized)\n",
    "    sharpe = np.sqrt(252) * daily_pnl.mean() / (daily_pnl.std() + 1e-8)\n",
    "    # Max Drawdown\n",
    "    max_dd = (cumulative / cumulative.cummax() - 1).min()\n",
    "    # CAGR (annualized)\n",
    "    total_days = (cumulative.index[-1] - cumulative.index[0]).days\n",
    "    cagr = cumulative.iloc[-1]**(365/total_days) - 1\n",
    "    # Total return\n",
    "    total_return = cumulative.iloc[-1] - 1\n",
    "\n",
    "    return {\n",
    "        'Sharpe': sharpe,\n",
    "        'Max Drawdown': max_dd,\n",
    "        'CAGR': cagr,\n",
    "        'Total Return': total_return\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f2170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Trained 2020-01-10 00:00:00 → 2020-04-06 00:00:00 | Tested 2020-04-07 00:00:00 → 2020-05-05 00:00:00 | Trades: 1721\n",
      "✅ Trained 2020-02-10 00:00:00 → 2020-05-05 00:00:00 | Tested 2020-05-06 00:00:00 → 2020-06-03 00:00:00 | Trades: 616\n",
      "✅ Trained 2020-03-10 00:00:00 → 2020-06-03 00:00:00 | Tested 2020-06-04 00:00:00 → 2020-07-01 00:00:00 | Trades: 767\n",
      "✅ Trained 2020-04-07 00:00:00 → 2020-07-01 00:00:00 | Tested 2020-07-02 00:00:00 → 2020-07-30 00:00:00 | Trades: 189\n",
      "✅ Trained 2020-05-06 00:00:00 → 2020-07-30 00:00:00 | Tested 2020-07-31 00:00:00 → 2020-08-27 00:00:00 | Trades: 202\n",
      "✅ Trained 2020-06-04 00:00:00 → 2020-08-27 00:00:00 | Tested 2020-08-28 00:00:00 → 2020-09-25 00:00:00 | Trades: 345\n",
      "✅ Trained 2020-07-02 00:00:00 → 2020-09-25 00:00:00 | Tested 2020-09-28 00:00:00 → 2020-10-23 00:00:00 | Trades: 52\n",
      "✅ Trained 2020-07-31 00:00:00 → 2020-10-23 00:00:00 | Tested 2020-10-26 00:00:00 → 2020-11-20 00:00:00 | Trades: 173\n",
      "✅ Trained 2020-08-28 00:00:00 → 2020-11-20 00:00:00 | Tested 2020-11-23 00:00:00 → 2020-12-21 00:00:00 | Trades: 144\n",
      "{'Sharpe': np.float64(0.8183616301690337), 'Max Drawdown': np.float64(-0.27365397377501555), 'CAGR': np.float64(0.2484516742953633), 'Total Return': np.float64(0.16982337670958203)}\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "results = rolling_backtest_calendar(df, feature_cols)\n",
    "results.to_csv(r'C:\\Users\\mrity\\OneDrive\\Desktop\\Alternative-Data-Alpha-Mining\\data\\backtest_result')\n",
    "metrics = compute_portfolio_metrics(results)\n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
